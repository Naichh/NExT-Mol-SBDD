{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/share/liuzhiyuan/envs/nextmol/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import warnings\n",
    "import lightning as L\n",
    "import lightning.pytorch.callbacks as plc\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from data_provider.data_module import QM9DataModule, QM9LMDataModule, GeomDrugsLMDataModule\n",
    "from data_provider.geom_drugs_jodo_dm import GeomDrugsJODODM\n",
    "from model.llm_pl import LLMPL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 正在初始化 QM9LMDataset ---\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# 5. RDKit对象\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m属性 \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrdmol\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m 的类型: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(first_molecule_data\u001b[38;5;241m.\u001b[39mrdmol)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m \u001b[43minspect_one_molecule\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m, in \u001b[0;36minspect_one_molecule\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- 正在初始化 QM9LMDataset ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 我们不需要tokenizer和rand_smiles，因为我们只是看原始数据\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mQM9LMDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m数据集加载成功！\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- 取出数据集中的第一个分子 (idx=0) ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/zyliu/nai/NExT-Mol/data_provider/qm9_dataset_v6.py:599\u001b[0m, in \u001b[0;36mQM9LMDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, selfies_tokenizer, rand_smiles, addHs, aug_inv)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, root: \u001b[38;5;28mstr\u001b[39m, transform: Optional[Callable] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    597\u001b[0m              pre_transform: Optional[Callable] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    598\u001b[0m              pre_filter: Optional[Callable] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, selfies_tokenizer: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, rand_smiles\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, addHs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, aug_inv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 599\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mQM9LMDataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_filter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselfies_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrand_smiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maddHs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maug_inv \u001b[38;5;241m=\u001b[39m aug_inv\n",
      "File \u001b[0;32m~/zyliu/nai/NExT-Mol/data_provider/qm9_dataset_v6.py:286\u001b[0m, in \u001b[0;36mQM9.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, selfies_tokenizer, rand_smiles, addHs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform, pre_transform, pre_filter)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_paths[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_unseen_selfies_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselfies_tokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/zyliu/nai/NExT-Mol/data_provider/qm9_dataset_v6.py:291\u001b[0m, in \u001b[0;36mQM9.add_unseen_selfies_tokens\u001b[0;34m(self, tokenizer)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_paths[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    290\u001b[0m     unseen_tokens \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[0;32m--> 291\u001b[0m vocab \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocab\u001b[49m()\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m unseen_tokens:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m vocab:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_vocab'"
     ]
    }
   ],
   "source": [
    "from data_provider.data_module import QM9LMDataset\n",
    "\n",
    "def inspect_one_molecule():\n",
    "    # --- 配置 ---\n",
    "    root_path = '/mnt/rna01/liuzhiyuan/zyliu/nai/NExT-Mol/data/GEOM-QM9'\n",
    "    tokenizer = LLMPL.init_tokenizer(args)\n",
    "\n",
    "    print(\"--- 正在   初始化 QM9LMDataset ---\")\n",
    "    # 我们不需要tokenizer和rand_smiles，因为我们只是看原始数据\n",
    "    dataset = QM9LMDataset(root=root_path)\n",
    "    print(\"数据集加载成功！\\n\")\n",
    "\n",
    "    print(\"--- 取出数据集中的第一个分子 (idx=0) ---\")\n",
    "    # 直接像访问列表一样，调用__getitem__(0)\n",
    "    first_molecule_data = dataset[0]\n",
    "\n",
    "    print(\"成功获取一个Data对象！\\n\")\n",
    "\n",
    "    print(\"--- 开始'解剖'这个Data对象 ---\")\n",
    "    # torch_geometric 会将Data对象打印得非常清晰\n",
    "    print(first_molecule_data)\n",
    "\n",
    "    print(\"\\n--- 访问具体属性 ---\")\n",
    "\n",
    "    # 1. 3D坐标\n",
    "    print(f\"属性 'pos' (3D坐标) 的形状: {first_molecule_data.pos.shape}\")\n",
    "    print(\"这代表了 [原子数量, 3(x,y,z)]\\n\")\n",
    "\n",
    "    # 2. 1D蓝图\n",
    "    print(f\"属性 'selfies' (1D蓝图): {first_molecule_data.selfies}\\n\")\n",
    "\n",
    "    # 3. 目标属性\n",
    "    print(f\"属性 'y' (19个化学属性) 的形状: {first_molecule_data.y.shape}\")\n",
    "    print(f\"第一个属性值(偶极矩): {first_molecule_data.y[0, 0].item():.4f}\\n\")\n",
    "\n",
    "    # 4. 图连接性\n",
    "    print(f\"属性 'edge_index' (图连接性) 的形状: {first_molecule_data.edge_index.shape}\")\n",
    "    print(\"这代表了 [2, 化学键数量 * 2]\\n\")\n",
    "\n",
    "    # 5. RDKit对象\n",
    "    print(f\"属性 'rdmol' 的类型: {type(first_molecule_data.rdmol)}\")\n",
    "inspect_one_molecule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 正在加载与 'acharkq/MoLlama' 配套的 Tokenizer ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tokenizer 对象本身 ---\n",
      "LlamaTokenizerFast(name_or_path='acharkq/MoLlama', vocab_size=4, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"[CH1-1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t5: AddedToken(\"[=S@@]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t6: AddedToken(\"[/F]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t7: AddedToken(\"[/123I]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t8: AddedToken(\"[C-1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t9: AddedToken(\"[C@]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t10: AddedToken(\"[/CH1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t11: AddedToken(\"[Cl]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t12: AddedToken(\"[#Branch1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t13: AddedToken(\"[/C@@H1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t14: AddedToken(\"[/C@@]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t15: AddedToken(\"[C@@H1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t16: AddedToken(\"[/P@@]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t17: AddedToken(\"[B@@-1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t18: AddedToken(\"[\\O]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t19: AddedToken(\"[N]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t20: AddedToken(\"[O]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t21: AddedToken(\"[/S@@+1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t22: AddedToken(\"[\\C@@H1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t23: AddedToken(\"[#N]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t24: AddedToken(\"[/NH1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t25: AddedToken(\"[=B]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t26: AddedToken(\"[\\F]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t27: AddedToken(\"[=NH0]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t28: AddedToken(\"[CH0]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t29: AddedToken(\"[C@H1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t30: AddedToken(\"[=O]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t31: AddedToken(\"[/C@]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t32: AddedToken(\"[=P@@H1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t33: AddedToken(\"[=Branch1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t34: AddedToken(\"[-\\Ring2]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t35: AddedToken(\"[=C]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t36: AddedToken(\"[127I]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t37: AddedToken(\"[\\Cl]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t38: AddedToken(\"[/Si]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t39: AddedToken(\"[125I]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t40: AddedToken(\"[#S]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t41: AddedToken(\"[CH1+1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t42: AddedToken(\"[=S@@+1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t43: AddedToken(\"[\\C]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t44: AddedToken(\"[=S@]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t45: AddedToken(\"[S@@H1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t46: AddedToken(\"[\\C@@]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t47: AddedToken(\"[/S+1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t48: AddedToken(\"[\\S]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t49: AddedToken(\"[\\C@H1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50: AddedToken(\"[F+1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t51: AddedToken(\"[PH2]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t52: AddedToken(\"[=Ring2]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t53: AddedToken(\"[=P@@]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t54: AddedToken(\"[BH1-1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t55: AddedToken(\"[/NH0]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t56: AddedToken(\"[B@@H1-1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t57: AddedToken(\"[S@@+1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t58: AddedToken(\"[O-1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t59: AddedToken(\"[P@@]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t60: AddedToken(\"[S]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t61: AddedToken(\"[S+1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t62: AddedToken(\"[Br]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t63: AddedToken(\"[I]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t64: AddedToken(\"[\\B-1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t65: AddedToken(\"[C+1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t66: AddedToken(\"[F]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t67: AddedToken(\"[/N]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t68: AddedToken(\"[/C-1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t69: AddedToken(\"[/O]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t70: AddedToken(\"[Ring1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t71: AddedToken(\"[OH0]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t72: AddedToken(\"[S@+1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t73: AddedToken(\"[=S]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t74: AddedToken(\"[\\P]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t75: AddedToken(\"[BH3-1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t76: AddedToken(\"[18OH1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t77: AddedToken(\"[/S@]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t78: AddedToken(\"[=CH0]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t79: AddedToken(\"[/P]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t80: AddedToken(\"[P]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t81: AddedToken(\"[P@H1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t82: AddedToken(\"[P+1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t83: AddedToken(\"[/I]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t84: AddedToken(\"[N-1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t85: AddedToken(\"[O+1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t86: AddedToken(\"[=P]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t87: AddedToken(\"[#P]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t88: AddedToken(\"[/CH1-1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t89: AddedToken(\"[\\123I]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t90: AddedToken(\"[Ring2]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t91: AddedToken(\"[BH2-1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t92: AddedToken(\"[\\N+1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t93: AddedToken(\"[S@]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t94: AddedToken(\"[P@@+1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t95: AddedToken(\"[\\S+1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t96: AddedToken(\"[=O+1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t97: AddedToken(\"[18F]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t98: AddedToken(\"[=P+1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t99: AddedToken(\"[SnH2]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t100: AddedToken(\"[=Branch2]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t101: AddedToken(\"[SH2]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t102: AddedToken(\"[SH3]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t103: AddedToken(\"[\\NH1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t104: AddedToken(\"[Branch1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t105: AddedToken(\"[=N]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t106: AddedToken(\"[=S@@H1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t107: AddedToken(\"[11CH3]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t108: AddedToken(\"[B]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t109: AddedToken(\"[SnH1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t110: AddedToken(\"[CH2-1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t111: AddedToken(\"[N@+1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t112: AddedToken(\"[/S]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t113: AddedToken(\"[\\P@@]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t114: AddedToken(\"[Sn]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t115: AddedToken(\"[CH1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t116: AddedToken(\"[\\S@]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t117: AddedToken(\"[=SH1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t118: AddedToken(\"[/C]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t119: AddedToken(\"[Si]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t120: AddedToken(\"[#N+1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t121: AddedToken(\"[B@H1-1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t122: AddedToken(\"[B-1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t123: AddedToken(\"[SH1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t124: AddedToken(\"[123I]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t125: AddedToken(\"[/B]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t126: AddedToken(\"[/CH0]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t127: AddedToken(\"[\\C-1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t128: AddedToken(\"[#Ring1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t129: AddedToken(\"[-/Ring1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t130: AddedToken(\"[NH1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t131: AddedToken(\"[N@@+1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t132: AddedToken(\"[=PH1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t133: AddedToken(\"[=S+1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t134: AddedToken(\"[PH1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t135: AddedToken(\"[Sn+3]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t136: AddedToken(\"[124I]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t137: AddedToken(\"[3H]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t138: AddedToken(\"[\\N]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t139: AddedToken(\"[C]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t140: AddedToken(\"[Branch2]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t141: AddedToken(\"[\\O-1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t142: AddedToken(\"[\\Br]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t143: AddedToken(\"[17F]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t144: AddedToken(\"[=N+1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t145: AddedToken(\"[\\C@]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t146: AddedToken(\"[CH2]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t147: AddedToken(\"[=P@H1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t148: AddedToken(\"[NH0]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t149: AddedToken(\"[/Br]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t150: AddedToken(\"[N+1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t151: AddedToken(\"[\\SH1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t152: AddedToken(\"[/N+1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t153: AddedToken(\"[\\CH1-1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t154: AddedToken(\"[S@@]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t155: AddedToken(\"[/Cl]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t156: AddedToken(\"[-/Ring2]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t157: AddedToken(\"[=P@]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t158: AddedToken(\"[P@+1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t159: AddedToken(\"[\\B]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t160: AddedToken(\"[/S@@]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t161: AddedToken(\"[=N-1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t162: AddedToken(\"[N@@H1+1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t163: AddedToken(\"[=Ring1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t164: AddedToken(\"[-\\Ring1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t165: AddedToken(\"[#C]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t166: AddedToken(\"[Br+1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t167: AddedToken(\"[=S@+1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t168: AddedToken(\"[\\Si]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t169: AddedToken(\"[\\S@@+1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t170: AddedToken(\"[P@@H1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t171: AddedToken(\"[/O-1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t172: AddedToken(\"[\\I]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t173: AddedToken(\"[C@@]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t174: AddedToken(\"[/OH0]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t175: AddedToken(\"[P@]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t176: AddedToken(\"[\\CH0]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t177: AddedToken(\"[/C@H1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t178: AddedToken(\"[#Branch2]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t179: AddedToken(\"[B@-1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t180: AddedToken(\"[\\S@@]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t181: AddedToken(\"[#C-1]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t182: AddedToken(\"[SH0]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t183: AddedToken(\"[Sn+2]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t184: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
      "}\n",
      "\n",
      "\n",
      "--- 查看特殊词元 (Special Tokens) ---\n",
      "起始符 (BOS): '<s>'  (ID: 0)\n",
      "结束符 (EOS): '<s>'  (ID: 0)\n",
      "填充符 (PAD): '<pad>'  (ID: 1)\n",
      "\n",
      "\n",
      "--- 查看部分'化学词汇表' (Vocabulary) ---\n",
      "词汇表总大小: 185 个词元\n",
      "一些化学片段示例:\n",
      "  - '[C]'  --> ID: 139\n",
      "  - '[=C]'  --> ID: 35\n",
      "  - '[Branch1]'  --> ID: 104\n",
      "  - '[Ring1]'  --> ID: 70\n",
      "  - '[O]'  --> ID: 20\n",
      "  - '[N]'  --> ID: 19\n",
      "\n",
      "\n",
      "--- 完整演示：从SELFIES到Batch输入 ---\n",
      "原始输入的SELFIES列表: ['[C][C][O]', '[C][=Branch1][C][=O]', '[F][C][F]']\n",
      "\n",
      "Tokenizer处理后的最终batch形态 (字典):\n",
      "{'input_ids': tensor([[  0, 139, 139,  20,   0,   1],\n",
      "        [  0, 139,  33, 139,  30,   0],\n",
      "        [  0,  66, 139,  66,   0,   1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0]])}\n",
      "\n",
      "\n",
      "--- 逐一解析batch内容 ---\n",
      "input_ids (形状: torch.Size([3, 6])):\n",
      "tensor([[  0, 139, 139,  20,   0,   1],\n",
      "        [  0, 139,  33, 139,  30,   0],\n",
      "        [  0,  66, 139,  66,   0,   1]])\n",
      "\n",
      "解读: 这是转换后的数字ID。每一行是一个分子。注意所有行的长度都一样（被填充到了最长序列的长度），并且开头是BOS的ID(1)，结尾是EOS的ID(2)。\n",
      "attention_mask (形状: torch.Size([3, 6])):\n",
      "tensor([[1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0]])\n",
      "\n",
      "解读: 值为1代表是真实数据，值为0代表是填充的。模型会根据这个来忽略填充部分。\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def haha():\n",
    "    # 我们要研究的模型名称\n",
    "    model_name = \"acharkq/MoLlama\"\n",
    "    print(f\"--- 正在加载与 '{model_name}' 配套的 Tokenizer ---\\n\")\n",
    "\n",
    "    # 1. 模拟 init_tokenizer 的核心步骤\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.add_bos_token = True\n",
    "    tokenizer.add_eos_token = True\n",
    "\n",
    "    print(\"--- Tokenizer 对象本身 ---\")\n",
    "    print(tokenizer)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"--- 查看特殊词元 (Special Tokens) ---\")\n",
    "    print(f\"起始符 (BOS): '{tokenizer.bos_token}'  (ID: {tokenizer.bos_token_id})\")\n",
    "    print(f\"结束符 (EOS): '{tokenizer.eos_token}'  (ID: {tokenizer.eos_token_id})\")\n",
    "    print(f\"填充符 (PAD): '{tokenizer.pad_token}'  (ID: {tokenizer.pad_token_id})\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"--- 查看部分'化学词汇表' (Vocabulary) ---\")\n",
    "    # 获取完整的 词 -> ID 映射字典\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    print(f\"词汇表总大小: {len(vocab)} 个词元\")\n",
    "\n",
    "    # 打印一些化学相关的词元\n",
    "    print(\"一些化学片段示例:\")\n",
    "    for token in ['[C]', '[=C]', '[Branch1]', '[Ring1]', '[O]', '[N]']:\n",
    "        if token in vocab:\n",
    "            print(f\"  - '{token}'  --> ID: {vocab[token]}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "    print(\"--- 完整演示：从SELFIES到Batch输入 ---\")\n",
    "    # 准备一批(batch)分子，长度不一\n",
    "    selfies_list = [\n",
    "        '[C][C][O]',                         # 乙醇 (3个词元)\n",
    "        '[C][=Branch1][C][=O]',              # 丙酮 (4个词元)\n",
    "        '[F][C][F]'                          # 二氟甲烷 (3个词元)\n",
    "    ]\n",
    "    print(f\"原始输入的SELFIES列表: {selfies_list}\\n\")\n",
    "\n",
    "    # 使用tokenizer进行处理，模拟LMCollater中的操作\n",
    "    batch = tokenizer(\n",
    "        selfies_list,\n",
    "        padding=True, # 自动填充到批次中最长的长度\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    print(\"Tokenizer处理后的最终batch形态 (字典):\")\n",
    "    print(batch)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"--- 逐一解析batch内容 ---\")\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "\n",
    "    print(f\"input_ids (形状: {input_ids.shape}):\\n{input_ids}\\n\")\n",
    "    print(\"解读: 这是转换后的数字ID。每一行是一个分子。注意所有行的长度都一样（被填充到了最长序列的长度），并且开头是BOS的ID(1)，结尾是EOS的ID(2)。\")\n",
    "\n",
    "    print(f\"attention_mask (形状: {attention_mask.shape}):\\n{attention_mask}\\n\")\n",
    "    print(\"解读: 值为1代表是真实数据，值为0代表是填充的。模型会根据这个来忽略填充部分。\")\n",
    "haha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 正在检查文件: 1m4n_A_rec_1m7y_ppg_lig_tt_min_0_pocket10.pkl ---\n",
      "\n",
      "--- 文件内容分析 ---\n",
      "加载出的数据顶层类型是: <class 'torch.Tensor'>\n",
      "文件内容直接是一个 PyTorch Tensor。\n",
      "\n",
      "--- 口袋嵌入(Embedding)详细信息 ---\n",
      "  - 数据类型 (dtype): torch.float32\n",
      "  - 形状 (Shape): torch.Size([65, 1536])\n",
      "  - 解读: 这代表了口袋中的 65 个实体（很可能是氨基酸残基），\n",
      "           每个实体被一个 1536 维的向量所描述。\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def inspect_sbd_pkl_file():\n",
    "    # --- 1. 配置区：请仔细修改以下路径 ---\n",
    "\n",
    "    pkl_file_path = '/mnt/rna01/liuzhiyuan/zyliu/nai/NExT-Mol/data/sbdd/crossdocked_pocket/1A1C_MALDO_2_433_0/1m4n_A_rec_1m7y_ppg_lig_tt_min_0_pocket10.pkl'\n",
    "\n",
    "    print(f\"--- 正在检查文件: {os.path.basename(pkl_file_path)} ---\")\n",
    "\n",
    "    if not os.path.exists(pkl_file_path):\n",
    "        print(f\"[错误] 文件未找到: {pkl_file_path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with open(pkl_file_path, 'rb') as f:\n",
    "            # 加载文件内容\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        print(\"\\n--- 文件内容分析 ---\")\n",
    "        print(f\"加载出的数据顶层类型是: {type(data)}\")\n",
    "\n",
    "        embedding_tensor = None\n",
    "        if isinstance(data, torch.Tensor):\n",
    "            # 情况A：文件直接就是一个张量\n",
    "            print(\"文件内容直接是一个 PyTorch Tensor。\")\n",
    "            embedding_tensor = data\n",
    "        elif isinstance(data, dict):\n",
    "            # 情况B：文件是一个字典，embedding在字典里面\n",
    "            print(f\"文件内容是一个字典，包含的键 (Keys): {list(data.keys())}\")\n",
    "            # 根据你同学写的Dataset代码，键名很可能是 'pdb_embedding'\n",
    "            if 'pdb_embedding' in data:\n",
    "                embedding_tensor = data['pdb_embedding']\n",
    "                print(\"在字典中找到了键 'pdb_embedding'。\")\n",
    "            else:\n",
    "                print(\"在字典中未找到预期的 'pdb_embedding' 键。\")\n",
    "        else:\n",
    "            print(\"文件内容既不是Tensor也不是字典，请检查具体内容。\")\n",
    "\n",
    "        # --- 3. 分析最终的Embedding张量 ---\n",
    "        if embedding_tensor is not None:\n",
    "            print(\"\\n--- 口袋嵌入(Embedding)详细信息 ---\")\n",
    "            print(f\"  - 数据类型 (dtype): {embedding_tensor.dtype}\")\n",
    "            print(f\"  - 形状 (Shape): {embedding_tensor.shape}\")\n",
    "\n",
    "            if len(embedding_tensor.shape) == 2:\n",
    "                num_residues, feature_dim = embedding_tensor.shape\n",
    "                print(f\"  - 解读: 这代表了口袋中的 {num_residues} 个实体（很可能是氨基酸残基），\")\n",
    "                print(f\"           每个实体被一个 {feature_dim} 维的向量所描述。\")\n",
    "            elif len(embedding_tensor.shape) == 1:\n",
    "                feature_dim = embedding_tensor.shape[0]\n",
    "                print(f\"  - 解读: 这是一个单一的、长度为 {feature_dim} 的向量，\")\n",
    "                print(f\"           它代表了整个口袋的整体特征（可能是通过平均池化得到的）。\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"读取或解析文件时出错: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    inspect_sbd_pkl_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "184087\n",
      "[('1B57_HUMAN_25_300_0/3vri_A_rec_3vrj_1kx_lig_tt_min_0_pocket10.pdb', '1B57_HUMAN_25_300_0/3vri_A_rec_3vrj_1kx_lig_tt_min_0.sdf', '1B57_HUMAN_25_300_0/3vri_A_rec.pdb', 0.524867), ('1B57_HUMAN_25_300_0/3upr_C_rec_5u98_1kx_lig_tt_min_0_pocket10.pdb', '1B57_HUMAN_25_300_0/3upr_C_rec_5u98_1kx_lig_tt_min_0.sdf', '1B57_HUMAN_25_300_0/3upr_C_rec.pdb', 0.402512), ('1B57_HUMAN_25_300_0/5u98_D_rec_5u98_1kx_lig_tt_min_0_pocket10.pdb', '1B57_HUMAN_25_300_0/5u98_D_rec_5u98_1kx_lig_tt_min_0.sdf', '1B57_HUMAN_25_300_0/5u98_D_rec.pdb', 0.367042), ('1B57_HUMAN_25_300_0/3upr_C_rec_3upr_1kx_lig_tt_docked_3_pocket10.pdb', '1B57_HUMAN_25_300_0/3upr_C_rec_3upr_1kx_lig_tt_docked_3.sdf', '1B57_HUMAN_25_300_0/3upr_C_rec.pdb', 0.319764), ('1B57_HUMAN_25_300_0/3upr_C_rec_3vri_1kx_lig_tt_min_0_pocket10.pdb', '1B57_HUMAN_25_300_0/3upr_C_rec_3vri_1kx_lig_tt_min_0.sdf', '1B57_HUMAN_25_300_0/3upr_C_rec.pdb', 0.360588)]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# 文件路径\n",
    "index_file = \"/data/share/liuzhiyuan/nai/NExT-Mol/datasets/sbdd/crossdocked_pocket/index.pkl\"\n",
    "\n",
    "# 打开并读取\n",
    "with open(index_file, 'rb') as f:\n",
    "    index_data = pickle.load(f)\n",
    "\n",
    "# 查看类型和内容\n",
    "print(type(index_data))\n",
    "print(len(index_data))\n",
    "print(index_data[:5])  # 如果是列表，查看前5条；如果是字典，可用 list(index_data.items())[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nextmol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
